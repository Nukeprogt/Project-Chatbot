{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import os\n",
    "\n",
    "# def load_knowledge_base(file_path: str) -> dict:\n",
    "#     if not os.path.exists(file_path):\n",
    "#         return {\"questions\": []}\n",
    "#     with open(file_path,'r') as file:\n",
    "#         data: dict = json.load(file)\n",
    "#     return data\n",
    "\n",
    "# def save_knowledge_base(file_path: str, data: dict):\n",
    "#     with open(file_path,'w') as file:\n",
    "#         json.dump(data, file, indent=2)\n",
    "\n",
    "# def find_best_match(user_question: str, questions: list[str], model) -> str | None:\n",
    "#     # Encode the user question and knowledge base questions into embeddings\n",
    "#     question_embeddings = model.encode(questions)\n",
    "#     user_embedding = model.encode(user_question)\n",
    "    \n",
    "#     # Compute cosine similarity between user input and stored questions\n",
    "#     similarities = util.pytorch_cos_sim(user_embedding, question_embeddings)\n",
    "    \n",
    "#     # Find the question with the highest similarity score\n",
    "#     best_match_index = similarities.argmax().item()\n",
    "#     best_match_score = similarities[0][best_match_index].item()\n",
    "\n",
    "#     # Return the best match if similarity is above a certain threshold (0.6)\n",
    "#     if best_match_score >= 0.6:\n",
    "#         return questions[best_match_index]\n",
    "#     return None\n",
    "\n",
    "# def get_answer_for_question(question: str, knowledge_base: dict) -> str | None:\n",
    "#     for q in knowledge_base[\"questions\"]:\n",
    "#         if q[\"question\"] == question:\n",
    "#             return q[\"answer\"]\n",
    "\n",
    "# def chat_bot():\n",
    "#     # Load knowledge base\n",
    "#     knowledge_base: dict = load_knowledge_base('knowledge_base.json')\n",
    "    \n",
    "#     # Initialize sentence transformer model\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')  # A small, fast model\n",
    "\n",
    "#     while True:\n",
    "#         user_inp = input('You: ')\n",
    "#         if user_inp.lower() == 'bye':\n",
    "#             print(f'Bot: Have a nice day <3')\n",
    "#             break\n",
    "        \n",
    "#         # Get a list of all questions in the knowledge base\n",
    "#         question_texts = [q[\"question\"] for q in knowledge_base[\"questions\"]]\n",
    "        \n",
    "#         # Find the best match using sentence embeddings\n",
    "#         best_match: str | None = find_best_match(user_inp, question_texts, model)\n",
    "        \n",
    "#         if best_match:\n",
    "#             # If a match is found, return the corresponding answer\n",
    "#             answer: str = get_answer_for_question(best_match, knowledge_base)\n",
    "#             print(f'Bot: {answer}')\n",
    "#         else:\n",
    "#             # If no match is found, prompt the user to teach the bot\n",
    "#             print('Bot: I don\\'t know the answer. Can you teach me?')\n",
    "#             new_ans: str = input('Type the answer or \"skip\" to skip: ')\n",
    "            \n",
    "#             if new_ans.lower() != 'skip':\n",
    "#                 # Add the new question-answer pair to the knowledge base\n",
    "#                 knowledge_base[\"questions\"].append({\"question\": user_inp, \"answer\": new_ans})\n",
    "#                 save_knowledge_base('knowledge_base.json', knowledge_base)\n",
    "#                 print('Bot: Thank you! I learned a new response')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     chat_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knowledge_base(file_path: str) -> dict:\n",
    "    if not os.path.exists(file_path):\n",
    "        return {\"questions\": []}\n",
    "    with open(file_path,'r') as file:\n",
    "        data: dict = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_knowledge_base(file_path: str, data: dict):\n",
    "    with open(file_path,'w') as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "def find_best_match(user_question: str, questions: list[str], model) -> str | None:\n",
    "    # Encode the user question and knowledge base questions into embeddings\n",
    "    question_embeddings = model.encode(questions)\n",
    "    user_embedding = model.encode(user_question)\n",
    "    \n",
    "    # Compute cosine similarity between user input and stored questions\n",
    "    similarities = util.pytorch_cos_sim(user_embedding, question_embeddings)\n",
    "    \n",
    "    # Find the question with the highest similarity score\n",
    "    best_match_index = similarities.argmax().item()\n",
    "    best_match_score = similarities[0][best_match_index].item()\n",
    "\n",
    "    # Return the best match if similarity is above a certain threshold (0.6)\n",
    "    if best_match_score >= 0.6:\n",
    "        return questions[best_match_index]\n",
    "    return None\n",
    "\n",
    "def get_answer_for_question(question: str, knowledge_base: dict) -> str | None:\n",
    "    for q in knowledge_base[\"questions\"]:\n",
    "        if q[\"question\"] == question:\n",
    "            return q[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kent.s\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBot: Thank you! I learned a new response\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mchat_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mchat_bot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# A small, fast model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     user_inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_inp\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBot: Have a nice day <3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def chat_bot():\n",
    "    # Load knowledge base\n",
    "    knowledge_base: dict = load_knowledge_base('knowledge_base.json')\n",
    "    \n",
    "    # Initialize sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # A small, fast model\n",
    "\n",
    "    while True:\n",
    "        user_inp = input('You: ')\n",
    "        if user_inp.lower() == 'bye':\n",
    "            print(f'Bot: Have a nice day <3')\n",
    "            break\n",
    "        \n",
    "        # Get a list of all questions in the knowledge base\n",
    "        question_texts = [q[\"question\"] for q in knowledge_base[\"questions\"]]\n",
    "        \n",
    "        # Find the best match using sentence embeddings\n",
    "        best_match: str | None = find_best_match(user_inp, question_texts, model)\n",
    "        \n",
    "        if best_match:\n",
    "            # If a match is found, return the corresponding answer\n",
    "            answer: str = get_answer_for_question(best_match, knowledge_base)\n",
    "            print(f'Bot: {answer}')\n",
    "        else:\n",
    "            # If no match is found, prompt the user to teach the bot\n",
    "            print('Bot: I don\\'t know the answer. Can you teach me?')\n",
    "            new_ans: str = input('Type the answer or \"skip\" to skip: ')\n",
    "            \n",
    "            if new_ans.lower() != 'skip':\n",
    "                # Add the new question-answer pair to the knowledge base\n",
    "                knowledge_base[\"questions\"].append({\"question\": user_inp, \"answer\": new_ans})\n",
    "                save_knowledge_base('knowledge_base.json', knowledge_base)\n",
    "                print('Bot: Thank you! I learned a new response')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chat_bot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
